{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247b4037-3aa9-4c35-bfe3-63d5206e0401",
   "metadata": {},
   "source": [
    "# Genetic Algorithm for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ba18c-73a3-4faf-8df2-fc2ad90a7c70",
   "metadata": {},
   "source": [
    "A Genetic Algorithm for feature selection is an evolutionary approach inspired by natural selection for selecting optimal feature subsets. The steps involve:\n",
    "\n",
    "1. Initialization: Generate an initial population of random feature subsets.\n",
    "2. Evaluation: Train a model using each feature subset and evaluate its performance \n",
    "3. Selection: Choose the best-performing feature subsets as parents for the next generation.\n",
    "4. Crossover: Combine pairs of parent feature subsets to create offspring by mixing their features.\n",
    "5. Mutation: Introduce small random changes to some offspring to maintain diversity and avoid local optima.\n",
    "6. Replacement: Replace the worst-performing feature subsets in the population with the new offspring.\n",
    "7. Iteration: Repeat the process until a stopping criterion is met\n",
    "\n",
    "Genetic Algorithms can capture complex, non-linear relationships between features, can be used with any type of model, and perform a comprehensive search of the feature space. The script is adapted from the tutorial found here https://medium.com/@ela.markovic/feature-selection-using-genetic-algorithm-complete-beginner-friendly-guide-198496393728 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e065933b-5130-4f91-8d7c-39d16e37c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from scipy.stats import ttest_rel\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import log_loss, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62506949-9628-4a94-92b1-1447f7f38cc7",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd96e1e-c6d0-4a96-a3e9-ffff23ca08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of features an individual (a molecule) can have\n",
    "min_features = 10 \n",
    "# Maximum number of features an individual (a molecule) can have\n",
    "max_features = 100 \n",
    "\n",
    "# The number of individuals in the population. This determines how many candidate solutions\n",
    "# (feature subsets) are considered in each generation.\n",
    "population_size = 100 \n",
    "\n",
    "max_iterations = 50 # Maximum number of generations\n",
    "\n",
    "# The percentage of the population that will be preserved as elite. Elite individuals\n",
    "# are kept without change and passed directly to the next generation.\n",
    "elite_percent = 0.6 \n",
    "\n",
    "# The probability that a mutation will occur in an individual. Mutation introduces variability\n",
    "# by flipping feature selections (0 to 1 or 1 to 0) within the individual.\n",
    "mutation_probability = 0.05 # Probability of mutation\n",
    "\n",
    "patience = 10  # For early stopping\n",
    "reevaluation_interval = 10  # Re-evaluate elites every 10 generations\n",
    "\n",
    "model_types = ['svm', 'random_forest', 'xgboost'] # List of model types to iterate over\n",
    "problem_types = ['classification','regression']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e3024-8cde-4385-8e1e-f8550a10d4d2",
   "metadata": {},
   "source": [
    "## Functions to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab113e-1a29-4228-bec4-9756f3483d52",
   "metadata": {},
   "source": [
    "### Create an initial population (step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850020f4-daf6-428e-a69d-a17e3a3be3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate random individuals (feature subsets)\n",
    "def generate_random_individuals(population_size, num_features, min_features, max_features):\n",
    "    \"\"\"\n",
    "    Generate a population of random individuals represented as binary vectors.\n",
    "\n",
    "    Parameters:\n",
    "    population_size (int): Number of individuals in the population.\n",
    "    num_features (int): Total number of features available.\n",
    "    min_features (int): Minimum number of features to include in an individual.\n",
    "    max_features (int): Maximum number of features to include in an individual.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D array where each row is a binary vector representing an individual.\n",
    "    \"\"\"\n",
    "    # Initialize the matrix with zeros. if a feature (column) in a population (row) is selected, it will be changed to 1\n",
    "    individuals = np.zeros((population_size, num_features)) \n",
    "    # Loop over the population (in this case, the compounds) and randomly select a set of features to keep \n",
    "    for i in range(population_size):\n",
    "        # Generate a random number of ones (random number of features to keep), which is between min_features and max_features\n",
    "        num_ones = np.random.randint(min_features, max_features + 1)\n",
    "        # Randomly assign the ones to features describing the compounds\n",
    "        ones_indices = np.random.choice(num_features, num_ones, replace=False) #indices that the ones will be assigned to\n",
    "        individuals[i, ones_indices] = 1 #assign the ones at the indices\n",
    "    return individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2af7a-aa05-4518-a59c-2a92b64f09f9",
   "metadata": {},
   "source": [
    "### Train a model on every chromosome from the population and calculate accuracy (steps 2 & 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a51c1-8424-4184-8f56-c869a2d2e95d",
   "metadata": {},
   "source": [
    "This `train_model` function is designed to train a specified regression model, evaluate its performance using cross-validation, and calculate an average R² score to assess model reliability. It starts by selecting only the predictor features specified by predictor_names from the training and test datasets. Based on the model type provided (`random_forest`, `xgboost`, `linear_regression`, or `svr`), the function initializes a model with built-in regularization to prevent overfitting: it limits tree depth in `RandomForestRegressor`, applies both L1 and L2 regularization terms in `XGBRegressor`, uses Ridge (L2) regularization for linear regression, and sets a regularization parameter `C` for `SVR`.\n",
    "\n",
    "The function then performs 5-fold cross-validation on the training data, obtaining an average R² score (`avg_cv_r2`) that provides a reliable performance estimate by reducing potential overfitting to any single subset of data. After cross-validation, the model is fit on the full training set and used to predict on the test set, yielding a test R² score (`test_r2`). Finally, an overall performance score (`final_score`) is calculated by averaging the cross-validation and test R² scores, giving a balanced measure of how well the model generalizes to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510a73aa-f7fc-4f9a-9945-51372942d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    x_train: pd.DataFrame, \n",
    "    x_val_split: pd.DataFrame, \n",
    "    y_train: pd.Series, \n",
    "    y_val_split: pd.Series, \n",
    "    predictor_names: list, \n",
    "    model_type: str, \n",
    "    problem_type: str\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train a specified model and calculate both loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x_train : pd.DataFrame\n",
    "        Training feature set.\n",
    "    x_val_split : pd.DataFrame\n",
    "        Validation feature set.\n",
    "    y_train : pd.Series\n",
    "        Training target values.\n",
    "    y_val_split : pd.Series\n",
    "        Validation target values.\n",
    "    predictor_names : list\n",
    "        List of feature names to include in the model.\n",
    "    model_type : str\n",
    "        The type of model ('random_forest', 'xgboost', 'svm').\n",
    "    problem_type : str\n",
    "        The problem type ('regression' or 'classification').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        Loss and accuracy scores.\n",
    "    \"\"\"\n",
    "    # Define model initialization dictionary\n",
    "    models = {\n",
    "        \"random_forest\": RandomForestClassifier(random_state=42) if problem_type == \"classification\" else RandomForestRegressor(random_state=42),\n",
    "        \"svm\": SVC(probability=True, random_state=42) if problem_type == \"classification\" else SVR(),\n",
    "        \"xgboost\": XGBClassifier(random_state=42) if problem_type == \"classification\" else XGBRegressor(random_state=42),\n",
    "    }\n",
    "\n",
    "    if model_type not in models:\n",
    "        raise ValueError(\"Invalid model_type. Choose from 'random_forest', 'xgboost', or 'svm'.\")\n",
    "\n",
    "    # Train model\n",
    "    mdl = models[model_type]\n",
    "    mdl.fit(x_train, y_train)\n",
    "    y_val_pred = mdl.predict(x_val_split)\n",
    "\n",
    "    if problem_type == \"classification\":\n",
    "        y_val_proba = mdl.predict_proba(x_val_split) if hasattr(mdl, \"predict_proba\") else None\n",
    "        all_labels = unique_labels(y_train, y_val_split)\n",
    "        \n",
    "        # Calculate loss and accuracy\n",
    "        loss = log_loss(y_val_split, y_val_proba, labels=all_labels) if y_val_proba is not None else 1 - accuracy_score(y_val_split, y_val_pred)\n",
    "        accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "\n",
    "    else:  # Regression\n",
    "        loss = mean_squared_error(y_val_split, y_val_pred)\n",
    "        accuracy = r2_score(y_val_split, y_val_pred)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a02cd7-e7f3-4e84-97f8-82ee394b6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fitness(\n",
    "    loss: float, \n",
    "    acc: float, \n",
    "    features_selected: int, \n",
    "    total_features: int, \n",
    "    alpha: float = 1.0, \n",
    "    beta: float = 1.0, \n",
    "    gamma: float = 0.004\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the fitness of an individual based on loss, accuracy, and feature sparsity.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    loss : float\n",
    "        Validation loss of the model (must be > 0).\n",
    "    acc : float\n",
    "        Validation accuracy of the model (range: 0-1).\n",
    "    features_selected : int\n",
    "        Number of features selected by the individual.\n",
    "    total_features : int\n",
    "        Total number of available features.\n",
    "    alpha : float, optional\n",
    "        Weight for the loss term (default is 1.0).\n",
    "    beta : float, optional\n",
    "        Weight for the accuracy term (default is 1.0).\n",
    "    gamma : float, optional\n",
    "        Weight for the sparsity term (default is 0.004).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        The fitness value of the individual.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if loss <= 0:\n",
    "        raise ValueError(\"Loss must be greater than zero to avoid division by zero.\")\n",
    "    if not (0 <= acc <= 1):\n",
    "        raise ValueError(\"Accuracy must be between 0 and 1.\")\n",
    "    if not (0 < features_selected <= total_features):\n",
    "        raise ValueError(\"Selected features must be between 1 and total features.\")\n",
    "\n",
    "    # Calculate sparsity\n",
    "    percent_features_ignored = (1 - features_selected / total_features) * 100\n",
    "\n",
    "    # Compute the fitness score\n",
    "    fitness = (\n",
    "        alpha / loss +            # Minimize loss\n",
    "        beta * acc +              # Maximize accuracy\n",
    "        gamma * percent_features_ignored  # Maximize sparsity\n",
    "    )\n",
    "    \n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5c386d-3edd-450c-af72-8a2627b4821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validated_fitness(\n",
    "    individual: np.ndarray, \n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series, \n",
    "    model_type: str, \n",
    "    problem_type: str, \n",
    "    k: int = 3, \n",
    "    alpha: float = 1.0, \n",
    "    beta: float = 1.0, \n",
    "    gamma: float = 0.004\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average fitness of an individual using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    individual : np.ndarray\n",
    "        Binary array representing the selected features.\n",
    "    X : pd.DataFrame\n",
    "        Feature dataset.\n",
    "    y : pd.Series\n",
    "        Target values.\n",
    "    model_type : str\n",
    "        Type of model ('random_forest', 'xgboost', 'svm', etc.).\n",
    "    problem_type : str\n",
    "        'regression' or 'classification'.\n",
    "    k : int, optional\n",
    "        Number of folds for cross-validation (default is 3).\n",
    "    alpha, beta, gamma : float, optional\n",
    "        Weights for loss, accuracy, and sparsity (defaults are 1.0, 1.0, and 0.004).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        Average fitness score across all folds. Returns -inf if no features are selected.\n",
    "    \"\"\"\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    scores = []\n",
    "\n",
    "    predictor_names = X.columns[individual == 1]\n",
    "    num_features_selected = len(predictor_names)\n",
    "\n",
    "    if num_features_selected == 0:\n",
    "        return -np.inf  # Invalid individual with no features selected\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "\n",
    "        # Ensure y is always a Pandas Series\n",
    "        y_train = y.iloc[train_idx] if isinstance(y, pd.Series) else y[train_idx]\n",
    "        y_val = y.iloc[val_idx] if isinstance(y, pd.Series) else y[val_idx]\n",
    "\n",
    "        try:\n",
    "            # Train model and calculate loss/accuracy\n",
    "            loss, acc = train_model(X_train, X_val, y_train, y_val, predictor_names, model_type, problem_type)\n",
    "\n",
    "            # Calculate fitness score\n",
    "            fitness = calculate_fitness(loss, acc, num_features_selected, len(X.columns), alpha, beta, gamma)\n",
    "            scores.append(fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model training: {e}\")\n",
    "            scores.append(-np.inf)\n",
    "\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c6cb2-04c6-4eea-b3da-7b4c4e17c9df",
   "metadata": {},
   "source": [
    "### Functions to Prevent Overfitting and Optimize Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e337a-1515-4953-86b8-89e8f9e23f18",
   "metadata": {},
   "source": [
    "This step ensures that elite individuals (top performers) are periodically validated using new data splits to detect overfitting. If an elite’s fitness drops significantly, it is replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55393f45-ca21-4160-ad42-e5462e3ed6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reevaluate_elites(\n",
    "    population: np.ndarray, \n",
    "    fitness: np.ndarray, \n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series, \n",
    "    model_type: str, \n",
    "    problem_type: str, \n",
    "    k: int = 3\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Re-evaluate the fitness of elites using new cross-validation splits.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    population : np.ndarray\n",
    "        Population of individuals (feature subsets).\n",
    "    fitness : np.ndarray\n",
    "        Fitness scores of the population.\n",
    "    X : pd.DataFrame\n",
    "        Feature dataset.\n",
    "    y : pd.Series\n",
    "        Target values.\n",
    "    model_type : str\n",
    "        Model type ('random_forest', 'xgboost', 'svm', etc.).\n",
    "    problem_type : str\n",
    "        'regression' or 'classification'.\n",
    "    k : int, optional\n",
    "        Number of folds for cross-validation (default is 3).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Updated fitness scores after re-evaluation.\n",
    "    \"\"\"\n",
    "    for i, individual in enumerate(population):\n",
    "        try:\n",
    "            # Recompute fitness for each individual\n",
    "            fitness[i] = cross_validated_fitness(individual, X, y, model_type, problem_type, k)\n",
    "        except Exception as e:\n",
    "            print(f\"Error re-evaluating individual {i}: {e}\")\n",
    "            fitness[i] = -np.inf  # Penalize invalid evaluations\n",
    "\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a152508-fd72-4041-a0ff-9f03aa0dcd32",
   "metadata": {},
   "source": [
    "Early stopping halts the algorithm if no significant improvement is observed over several generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597a1823-714b-4eb7-b956-52ab337534a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_early_stopping(\n",
    "    best_fitness: list[float], \n",
    "    stagnation_counter: int, \n",
    "    patience: int, \n",
    "    tolerance: float = 1e-5\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check whether early stopping should be triggered.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    best_fitness : list[float]\n",
    "        List of best fitness scores across generations.\n",
    "    stagnation_counter : int\n",
    "        Current number of stagnant generations.\n",
    "    patience : int\n",
    "        Maximum allowed stagnant generations for early stopping.\n",
    "    tolerance : float, optional\n",
    "        Minimum improvement required to consider progress (default is 1e-5).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    bool\n",
    "        True if early stopping is triggered, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(best_fitness) <= patience:\n",
    "        return False  # Not enough generations for comparison\n",
    "\n",
    "    # Check if there has been significant improvement in the last 'patience' generations\n",
    "    recent_fitness = best_fitness[-patience:]\n",
    "    if max(recent_fitness) - min(recent_fitness) < tolerance:\n",
    "        print(f\"Early stopping triggered after {len(best_fitness)} generations.\")\n",
    "        return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7804a0d-9ec8-4e5c-aa92-faa089f1739c",
   "metadata": {},
   "source": [
    "### Select parents (step 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238127e-be9a-4085-b1de-1bb08365bfa7",
   "metadata": {},
   "source": [
    "This function chooses parents for the next generation using a combination of elite selection (based on accuracy) and a roulette wheel selection strategy to ensure that well-performing individuals have a higher chance of being selected while maintaining diversity.\n",
    "\n",
    "The first few lines inside the function calculate the number of elite individuals to keep based on `elite_percent`, sort the individuals in the population in descending order of fitness scores using `np.argsort(-accuracy)`, and select the top `elite_num` individuals as the elite population. The calculation `elite_num = int(round(((elite_percent * population.shape[0]) // 2) * 2))` ensures that `elite_num` is always even so that each elite individual has a potential partner for mating. The `elite_population` array contains these elite individuals that will be preserved for the next generation without mutation.\n",
    "\n",
    "The rest of the parents are picked based on their accuracy scores using a roulette wheel selection strategy. The `accuracy` array holds the R^2 scores for each individual in the population. The process works as follows:\n",
    "1. The accuracy scores are normalized to create a weight distribution, and cumulative weights are calculated to represent the total fitness distribution.\n",
    "2. A random number between 0 and the total cumulative weight is generated to simulate the roulette wheel.\n",
    "3. The function uses `np.searchsorted` to find the index where the random number fits within the cumulative weights. This index corresponds to a selected chromosome, with higher accuracy scores having greater weights and thus a higher probability of selection.\n",
    "\n",
    "The function returns an array of chosen parents, which includes both the elite individuals and non-elite individuals selected by the roulette wheel method. This approach maintains a balance between retaining the best-performing solutions and introducing diversity for the next generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11993079-a6b2-4244-80d0-b45b6b3fc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parents(population: np.ndarray, fitness: np.ndarray, elite_percent: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Select parents for the next generation based on fitness using elitism and roulette selection.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    population : np.ndarray\n",
    "        Current population of individuals (feature subsets).\n",
    "    fitness : np.ndarray\n",
    "        Array of fitness scores for each individual.\n",
    "    elite_percent : float\n",
    "        Percentage of top individuals to preserve as elite.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Selected parents for the next generation.\n",
    "    \"\"\"\n",
    "    pop_size, num_features = population.shape\n",
    "\n",
    "    # Ensure elite_num is even for balanced crossover\n",
    "    elite_num = max(2, int(round((elite_percent * pop_size) // 2) * 2))\n",
    "\n",
    "    # Rank individuals by descending fitness\n",
    "    elite_indices = np.argsort(-fitness)[:elite_num]\n",
    "    elite_population = population[elite_indices]\n",
    "\n",
    "    # Normalize fitness for roulette wheel selection\n",
    "    if fitness.sum() == 0:\n",
    "        raise ValueError(\"All fitness scores are zero. Check model evaluation or fitness function.\")\n",
    "\n",
    "    # Perform roulette wheel selection\n",
    "    weight_norm = fitness / fitness.sum()\n",
    "    weight_cumsum = weight_norm.cumsum()\n",
    "\n",
    "    # Select non-elite parents\n",
    "    num_parents_wo_elite = pop_size - elite_num\n",
    "    parents_wo_elite = np.zeros((num_parents_wo_elite, num_features))\n",
    "\n",
    "    for i in range(num_parents_wo_elite):\n",
    "        rand_num = np.random.uniform(0, 1)\n",
    "        selected_idx = np.searchsorted(weight_cumsum, rand_num)\n",
    "        parents_wo_elite[i, :] = population[selected_idx, :]\n",
    "\n",
    "    # Combine elite and selected parents\n",
    "    parents = np.vstack((elite_population, parents_wo_elite))\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b23819-d8b9-4424-b615-bcecd1f80e50",
   "metadata": {},
   "source": [
    "### Generate children and mutate with probability (step 5 & 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4191797-0708-4e20-8988-86b26bded856",
   "metadata": {},
   "source": [
    "The function first calculates the number of elite individuals based on the `elite_percentinput`, and then selects the elite individuals to be directly passed on to the next generation. The remaining children are created using the one-point crossover method, where pairs of parents are selected and a random crossover point is chosen. The children are created by combining the first part of one parent and the second part of the other parent after the crossover point.\n",
    "\n",
    "After the crossover operation, the function checks if each child has between the minimum number of features and maximum number of features. If the number of 1s (true values) is greater than the maximum allowed number of features, some of the true values are randomly changed to false values. Similarly, if the number of 1s is less than the minimum allowed number of features, some of the false values are randomly changed to true values.\n",
    "\n",
    "Finally, the function performs mutation by randomly selecting some bits in the child and flipping them from 0 to 1 or from 1 to 0 based on the mutation probability and the number of features already present in the offspring.\n",
    "\n",
    "The output of the function is a 2D numpy array representing the child population after the one-point crossover and mutation operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a94cf3-21b4-48cc-9c18-7bba54af028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_point_crossover(\n",
    "    parents: np.ndarray, \n",
    "    elite_percent: float, \n",
    "    mutation_probability: float, \n",
    "    min_features: int, \n",
    "    max_features: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform one-point crossover and mutation to generate a new population.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    parents : np.ndarray\n",
    "        Array of selected parents for crossover.\n",
    "    elite_percent : float\n",
    "        Percentage of parents to keep as elite.\n",
    "    mutation_probability : float\n",
    "        Probability of mutation occurring in an individual.\n",
    "    min_features : int\n",
    "        Minimum number of features an individual can have.\n",
    "    max_features : int\n",
    "        Maximum number of features an individual can have.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        New population generated after crossover and mutation.\n",
    "    \"\"\"\n",
    "    pop_size, num_features = parents.shape\n",
    "\n",
    "    # Ensure elite count is even and calculate elite size\n",
    "    elite_num = max(2, int(round((elite_percent * pop_size) // 2) * 2))\n",
    "    new_population = np.zeros_like(parents)\n",
    "    new_population[:elite_num] = parents[:elite_num]  # Preserve elite individuals\n",
    "\n",
    "    # Perform one-point crossover\n",
    "    for i in range(elite_num, pop_size, 2):\n",
    "        if i + 1 >= pop_size:\n",
    "            break\n",
    "        p1, p2 = parents[i], parents[i + 1]\n",
    "        crossover_point = np.random.randint(1, num_features - 1)\n",
    "\n",
    "        # Create offspring\n",
    "        new_population[i] = np.concatenate([p1[:crossover_point], p2[crossover_point:]])\n",
    "        new_population[i + 1] = np.concatenate([p2[:crossover_point], p1[crossover_point:]])\n",
    "\n",
    "    # Adjust feature counts\n",
    "    for ind in range(pop_size):\n",
    "        feature_count = new_population[ind].sum()\n",
    "\n",
    "        if feature_count > max_features:\n",
    "            excess = int(feature_count - max_features)\n",
    "            indices_to_turn_off = np.random.choice(\n",
    "                np.where(new_population[ind] == 1)[0], size=excess, replace=False\n",
    "            )\n",
    "            new_population[ind, indices_to_turn_off] = 0\n",
    "\n",
    "        elif feature_count < min_features:\n",
    "            deficit = min_features - feature_count\n",
    "            indices_to_turn_on = np.random.choice(\n",
    "                np.where(new_population[ind] == 0)[0], size=deficit, replace=False\n",
    "            )\n",
    "            new_population[ind, indices_to_turn_on] = 1\n",
    "\n",
    "    # Apply mutations\n",
    "    num_mutations = int(new_population.size * mutation_probability)\n",
    "    for _ in range(num_mutations):\n",
    "        ind_row = np.random.randint(0, pop_size)\n",
    "        ind_col = np.random.randint(0, num_features)\n",
    "        current_value = new_population[ind_row, ind_col]\n",
    "\n",
    "        # Toggle feature with constraint checks\n",
    "        if current_value == 0 and new_population[ind_row].sum() < max_features:\n",
    "            new_population[ind_row, ind_col] = 1\n",
    "        elif current_value == 1 and new_population[ind_row].sum() > min_features:\n",
    "            new_population[ind_row, ind_col] = 0\n",
    "\n",
    "    return new_population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95740b50-b586-44ce-926f-c0c771210c97",
   "metadata": {},
   "source": [
    "## Add Dummy Variable and Save to csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "770c3818-cbbc-46df-bde8-7cb00186f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_all_files(\n",
    "    input_dir: str, \n",
    "    output_dir: str, \n",
    "    selected_columns: list, \n",
    "    descriptor_file: str, \n",
    "    model_type: str, \n",
    "    problem_type: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Process input files by merging, selecting features, and saving processed files.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        Directory containing input CSV files.\n",
    "    output_dir : str\n",
    "        Directory to save processed CSV files.\n",
    "    selected_columns : list\n",
    "        List of reduced feature names to retain.\n",
    "    descriptor_file : str\n",
    "        Path to descriptors_all.csv for reference.\n",
    "    model_type : str\n",
    "        Model type used for feature selection (e.g., 'linear_regression').\n",
    "    problem_type : str\n",
    "        Either 'regression' or 'classification'.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    descriptor_df = pd.read_csv(descriptor_file)\n",
    "    file_pattern = \"*_reg*.csv\" if problem_type == \"regression\" else \"*_class*.csv\"\n",
    "    files = glob.glob(os.path.join(input_dir, file_pattern))\n",
    "\n",
    "    for file_path in files:\n",
    "        input_df = pd.read_csv(file_path).set_index(\"Molecule ChEMBL ID\")\n",
    "\n",
    "        if problem_type == \"regression\":\n",
    "            merged_df = pd.merge(\n",
    "                descriptor_df[[\"Molecule ChEMBL ID\", \"-logIC50\"]],\n",
    "                input_df,\n",
    "                on=\"Molecule ChEMBL ID\",\n",
    "                how=\"inner\"\n",
    "            )\n",
    "\n",
    "            if \"-logIC50\" in merged_df.columns:\n",
    "                merged_df['is_imputed'] = np.isclose(\n",
    "                    merged_df['-logIC50'], -np.log(10000 * 1e-9), atol=1e-6\n",
    "                ).astype(int)\n",
    "                processed_df = merged_df.loc[:, selected_columns + ['is_imputed']]\n",
    "                processed_df.drop(columns=\"-logIC50\", inplace=True, errors=\"ignore\")\n",
    "            else:\n",
    "                processed_df = merged_df[selected_columns]\n",
    "        else:  # Classification\n",
    "            processed_df = input_df[selected_columns]\n",
    "\n",
    "        if not processed_df.empty:\n",
    "            output_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "            processed_df.to_csv(output_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0bf59-8054-487b-8112-e362b3cefecd",
   "metadata": {},
   "source": [
    "## Read inputs and run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce94bc1-fdb5-4bd8-a501-a830c3a4bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress for svm:  14%|████████████████▊                                                                                                     | 7/49 [13:39<1:21:58, 117.10s/gen]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize label encoder only when required\n",
    "label_encoder = None  \n",
    "\n",
    "for model_type in model_types:\n",
    "    for problem_type in problem_types:\n",
    "        # Load appropriate dataset\n",
    "        data_file = (\n",
    "            \"../../3_train_test_split/train_reg.csv\" if problem_type == \"regression\" \n",
    "            else \"../../3_train_test_split/train_class.csv\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Load features and target values\n",
    "            data = pd.read_csv(data_file).set_index(\"Molecule ChEMBL ID\")\n",
    "            y = pd.read_csv(\"../../3_train_test_split/descriptors_all.csv\").set_index(\"Molecule ChEMBL ID\")\n",
    "            \n",
    "            target_column = \"-logIC50\" if problem_type == \"regression\" else \"Potency\"\n",
    "            merged = pd.merge(data, y[[target_column]], left_index=True, right_index=True, how=\"inner\")\n",
    "            X, y = merged.drop(columns=target_column), merged[target_column]\n",
    "\n",
    "            # Encode target labels for classification\n",
    "            if problem_type == \"classification\":\n",
    "                label_encoder = LabelEncoder()\n",
    "                y = label_encoder.fit_transform(y.str.strip())\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check for mismatches between X and y\n",
    "        if X.shape[0] != len(y):\n",
    "            print(f\"Warning: Feature-target mismatch: {X.shape[0]} vs {len(y)}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize genetic algorithm variables\n",
    "        num_features = X.shape[1]\n",
    "        population = generate_random_individuals(population_size, num_features, min_features, max_features)\n",
    "        fitness = np.zeros(population_size)\n",
    "        predictor_names = X.columns\n",
    "        stagnation_counter = 0\n",
    "        best_fitness_i = np.zeros(max_iterations)\n",
    "        best_fitness_overall = -np.inf\n",
    "\n",
    "        # Evaluate Initial Population\n",
    "        fitness = np.array([\n",
    "            cross_validated_fitness(ind, X, y, model_type, problem_type, k=3)\n",
    "            for ind in population\n",
    "        ])\n",
    "        \n",
    "        best_fitness_i[0] = fitness.max()\n",
    "        best_fitness_overall = fitness.max()\n",
    "\n",
    "        # Run Genetic Algorithm\n",
    "        with tqdm(total=max_iterations - 1, desc=f'Progress for {model_type}', unit='gen') as pbar:\n",
    "            for gen in range(1, max_iterations):\n",
    "                # Select parents and generate offspring\n",
    "                parents = choose_parents(population, fitness, elite_percent)\n",
    "                children = one_point_crossover(\n",
    "                    parents, elite_percent, mutation_probability, min_features, max_features\n",
    "                )\n",
    "                population = children\n",
    "\n",
    "                # Evaluate new population\n",
    "                fitness = np.array([\n",
    "                    cross_validated_fitness(ind, X, y, model_type, problem_type, k=3)\n",
    "                    for ind in population\n",
    "                ])\n",
    "                \n",
    "                best_fitness_i[gen] = fitness.max()\n",
    "                if fitness.max() > best_fitness_overall:\n",
    "                    best_fitness_overall = fitness.max()\n",
    "                    stagnation_counter = 0\n",
    "                else:\n",
    "                    stagnation_counter += 1\n",
    "\n",
    "                if stagnation_counter >= patience:\n",
    "                    print(\"Early stopping triggered due to stagnation.\")\n",
    "                    break\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Save Best Model and Results\n",
    "        best_individual_idx = fitness.argmax()\n",
    "        best_features = population[best_individual_idx]\n",
    "        best_feature_names = predictor_names[best_features == 1]\n",
    "\n",
    "        process_and_save_all_files(\n",
    "            \".\", model_type, best_feature_names, \n",
    "            \"../../3_train_test_split/descriptors_all.csv\", \n",
    "            model_type, problem_type\n",
    "        )\n",
    "\n",
    "        # Save and Plot Results\n",
    "        os.makedirs(\"plots\", exist_ok=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(1, gen + 2), best_fitness_i[:gen + 1], marker='o')\n",
    "        plt.xlabel(\"Number of Generations\")\n",
    "        plt.ylabel(\"Best Fitness Score\")\n",
    "        plt.title(f\"Genetic Algorithm for Feature Selection - {model_type}\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        plt.savefig(f'plots/genetic_algorithm_plot_{model_type}_{problem_type}.png', dpi=150)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
